<!DOCTYPE html><html lang="en-us"><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>Why LLM Users Still Need Apps - Decent Apps</title><meta name="description" content="The theory others propose: it will be the 'Death of Apps' tomorrow, or maybe next Tuesday. We will arrive at a place where users speak their wishes to a well-connected LLM, and it just gets done. No knob twiddling, button pushing, or even a glance&hellip;"><meta name="generator" content="Publii Open-Source CMS for Static Site"><link rel="canonical" href="./why-llms-still-need-apps.html"><link rel="shortcut icon" href="./media/website/decentLogo16.png" type="image/png"><link rel="preload" href="./assets/dynamic/fonts/instrumentsans/instrumentsans.woff2" as="font" type="font/woff2" crossorigin><link rel="preload" href="./assets/dynamic/fonts/instrumentsans/instrumentsans-italic.woff2" as="font" type="font/woff2" crossorigin><link rel="preload" href="./assets/dynamic/fonts/jellee/jellee.woff2" as="font" type="font/woff2" crossorigin><link rel="stylesheet" href="./assets/css/style.css?v=dae9d45edfbd0d6454b7815fc9d71a2e"><script type="application/ld+json">{"@context":"http://schema.org","@type":"Article","mainEntityOfPage":{"@type":"WebPage","@id":"./why-llms-still-need-apps.html"},"headline":"Why LLM Users Still Need Apps","datePublished":"2025-03-12T16:45-07:00","dateModified":"2025-03-23T10:56-07:00","image":{"@type":"ImageObject","url":"./media/posts/2/llmAndApp2.jpg","height":640,"width":1904},"description":"The theory others propose: it will be the 'Death of Apps' tomorrow, or maybe next Tuesday. We will arrive at a place where users speak their wishes to a well-connected LLM, and it just gets done. No knob twiddling, button pushing, or even a glance&hellip;","author":{"@type":"Person","name":"Erik Hermansen","url":"./authors/erik-hermansen/"},"publisher":{"@type":"Organization","name":"Erik Hermansen","logo":{"@type":"ImageObject","url":"./media/website/decentLogo192.png","height":192,"width":192}}}</script><noscript><style>img[loading] {
                    opacity: 1;
                }</style></noscript></head><body class="post-template"><header class="top js-header" style="background-color:#a58c75"><a class="logo" href="./"><img src="./media/website/decentLogo192.png" alt="Decent Apps" width="192" height="192"></a><nav class="navbar js-navbar"><button class="navbar__toggle js-toggle" aria-label="Menu" aria-haspopup="true" aria-expanded="false"><span class="navbar__toggle-box"><span class="navbar__toggle-inner">Menu</span></span></button><ul class="navbar__menu"><li class="has-submenu"><span class="is-separator" aria-haspopup="true">Decent Portal</span><ul class="navbar__submenu level-2" aria-hidden="true"><li><a href="./decent-portal-coming-soon.html" target="_self">General Information</a></li><li><a href="./hone-a-local-llm-spreadsheet-tool-built-on-decent.html" target="_self">Hone (a Decent app)</a></li></ul></li><li class="has-submenu"><span class="is-separator" aria-haspopup="true">Developer Resources</span><ul class="navbar__submenu level-2" aria-hidden="true"><li><a href="./decent-portal-app-requirements.html" target="_self">Decent Portal App Requirements</a></li><li><a href="./partner-deployment-guide.html" target="_self">Partner Deployment Guide</a></li><li><a href="./create-decent-app-quick-start.html" target="_self">Create-Decent-App Quick Start</a></li><li><a href="https://github.com/DecentAppsNet" target="_blank">Decent Apps Github</a></li><li><a href="https://discord.gg/kkp3x4X2Vb" target="_blank">Decent Apps Discord</a></li></ul></li><li><a href="./tags/blog/" target="_self">Blog</a></li><li><a href="./contact-us.html" target="_self">Contact Us</a></li></ul></nav></header><main class="post"><article class="content"><div class="hero"><header class="hero__content"><div class="wrapper"><h1>Why LLM Users Still Need Apps</h1><div class="feed__meta content__meta"><a href="./authors/erik-hermansen/" class="feed__author">Erik Hermansen</a> <time datetime="2025-03-12T16:45" class="feed__date">March 12, 2025</time></div></div></header><figure class="hero__image"><div class="hero__image-wrapper"><img src="./media/posts/2/llmAndApp2.jpg" srcset="./media/posts/2/responsive/llmAndApp2-xs.jpg 640w, ./media/posts/2/responsive/llmAndApp2-sm.jpg 768w, ./media/posts/2/responsive/llmAndApp2-md.jpg 1024w, ./media/posts/2/responsive/llmAndApp2-lg.jpg 1366w, ./media/posts/2/responsive/llmAndApp2-xl.jpg 1600w, ./media/posts/2/responsive/llmAndApp2-2xl.jpg 1920w" sizes="88vw" loading="eager" height="640" width="1904" alt="Illustration of robot viewing a laptop"></div></figure></div><div class="entry-wrapper content__entry"><p><strong>The theory others propose: </strong>it will be the "Death of Apps" tomorrow, or maybe next Tuesday. We will arrive at a place where users speak their wishes to a well-connected LLM, and it just gets done. No knob twiddling, button pushing, or even a glance at the UI of an application.</p><p>While I am no naysayer of conversational interfaces, I don't think app interfaces are going away. I'll give you the two big reasons.</p><h3>1. Apps Deliver Precision Interfaces for Users</h3><p>I use Krita, (<a href="https://krita.org/" target="_blank" rel="noopener noreferrer">their website</a>) an open-source Photoshop-style app, to create images. Its screen has a UI that is something like the cockpit of an airplane. Each little icon I might press needs at least 5 minutes of newbie time to learn how it works. And there are countless panels and dialogs that expand the possibility space of Krita well beyond what is immediately visible within its UI.</p><figure class="post__image post__image--wide"><img loading="lazy" src="./media/posts/2/kritaExample.jpg" alt="" width="2486" height="1570" sizes="(max-width: 1920px) 100vw, 1920px" srcset="./media/posts/2/responsive/kritaExample-xs.jpg 640w, ./media/posts/2/responsive/kritaExample-sm.jpg 768w, ./media/posts/2/responsive/kritaExample-md.jpg 1024w, ./media/posts/2/responsive/kritaExample-lg.jpg 1366w, ./media/posts/2/responsive/kritaExample-xl.jpg 1600w, ./media/posts/2/responsive/kritaExample-2xl.jpg 1920w"></figure><p>I love Krita, but compared to the glorious AI future on offer, it does seem pretty complex and tedious, right?</p><p>Could we not just lounge in our chair while being fed grapes and call to our AI servant, "Craft me an image. And be quick about it!"</p><p>For the image above, that's basically how I started using Midjourney. You can see one of the more successful prompts below that I built from.</p><figure class="post__image post__image--wide"><img loading="lazy" src="./media/posts/2/midJourneyExample.jpg" alt="" width="2994" height="1130" sizes="(max-width: 1920px) 100vw, 1920px" srcset="./media/posts/2/responsive/midJourneyExample-xs.jpg 640w, ./media/posts/2/responsive/midJourneyExample-sm.jpg 768w, ./media/posts/2/responsive/midJourneyExample-md.jpg 1024w, ./media/posts/2/responsive/midJourneyExample-lg.jpg 1366w, ./media/posts/2/responsive/midJourneyExample-xl.jpg 1600w, ./media/posts/2/responsive/midJourneyExample-2xl.jpg 1920w"></figure><p>Midjourney did a fine job of generating different candidate images matching my prompt. I picked one and continued refining. It's important to note that Midjourney is very much an <em>app</em> that allows specification beyond prompting. I used UI-presented features like upscaling, regional masking, and character references to arrive at a base image that showed Bill Gates handing a bag of trash to the heroic woman from Apple's <a href="https://www.youtube.com/watch?v=ErwS24cBZPc" target="_blank" rel="noopener noreferrer">"1984" commercial</a> (YouTube). And then I overlayed text in the exact font and position I wanted using Krita.</p><p>My point is not to impress you with my dime-a-dozen photo manipulation skills. Rather I am calling out the usefulness of application UI when combined with generative AI.</p><p>As a user, you can say what you want in language, and that feels quite natural. But just because speaking or writing is a familiar activity to us, doesn't mean it's the most efficient way to specify every desired outcome. The visual interface of drawing a rectangle over the blonde man's head to select it for replacement is fast, precise, and hard to beat with a conversational interface.</p><figure class="post__image post__image--wide"><img loading="lazy" src="./media/posts/2/crayonColors.jpg" alt="" width="1904" height="640" sizes="(max-width: 1920px) 100vw, 1920px" srcset="./media/posts/2/responsive/crayonColors-xs.jpg 640w, ./media/posts/2/responsive/crayonColors-sm.jpg 768w, ./media/posts/2/responsive/crayonColors-md.jpg 1024w, ./media/posts/2/responsive/crayonColors-lg.jpg 1366w, ./media/posts/2/responsive/crayonColors-xl.jpg 1600w, ./media/posts/2/responsive/crayonColors-2xl.jpg 1920w"></figure><p>Imagine choosing a specific color. Do you really want to have a conversation about the desired color? ("Uh... a stormy kind of red or actually... mauve, but not a dark mauve...") Or do you want to pick it from a swatch? If I care about the color, I'll take the swatch. Maybe if I memorized the names of all those Crayola crayons when I was a kid, I'd feel different.</p><p>The second reason why we need apps for LLM use cases is even more important.</p><h3>2. Full Automation with LLMs is Usually a Bad Idea</h3><p>The dream that seems to fund a thousand AI startups is full automation. Ask the bot to build a website, book a flight, write a legal document, invest in stocks, and the bot just handles it for you. And you can spend all your free time watching your favorite AI-generated movies.</p><p>But due to hallucinations and errors, we don't actually want LLMs to do our work in a fully automated way. Despite promises that the accuracy problems are going away, I believe it will take a real breakthrough to get there - not just interative improvements to LLMs with more scaling and training. And you don't put AI breakthroughs as a scheduled event on a roadmap. They happen when they happen.</p><figure class="post__image post__image--wide"><img loading="lazy" src="./media/posts/2/fullAutomationBad.jpg" alt="image of out of control conveyer belt " width="1904" height="640" sizes="(max-width: 1920px) 100vw, 1920px" srcset="./media/posts/2/responsive/fullAutomationBad-xs.jpg 640w, ./media/posts/2/responsive/fullAutomationBad-sm.jpg 768w, ./media/posts/2/responsive/fullAutomationBad-md.jpg 1024w, ./media/posts/2/responsive/fullAutomationBad-lg.jpg 1366w, ./media/posts/2/responsive/fullAutomationBad-xl.jpg 1600w, ./media/posts/2/responsive/fullAutomationBad-2xl.jpg 1920w"></figure><p>Depending on the task and model, your LLM is going to be right 50% to 99% of the time, but never 100%. If the task has a consequential result, (e.g., booking an airplane flight) you must have a human at least verifying the proposed action before it is performed. And that brings us back to the usefulness of apps with their UIs for presenting information and interacting with users.</p><p>There are also times when work performed by an LLM can be simplied by the structure of an app. It's a very challenging problem for an LLM+RAG system to index and retrieve a large corporation's tens of thousands of documents. (I believe nobody is doing it well currently, including Microsoft.)</p><p>But if you had an app where the user curated a list of documents that were important just to themself, the LLM+RAG system need only deal with hundreds of documents. And docs from that smaller collection can be retrieved with more accuracy. This simple trick of limiting the context to something smaller is key to handling LLM use cases successfully.</p><h3>Setting Sights on What is Possible Now</h3><p>Rather than assume apps will be wiped out by "just do it" commands given to bots, let's meet the technology where it is right now. We can make LLM-based apps do useful things for actual people, rather than make claims about what we might eventually be able to do. With this practical attitude, there are countless opportunities to create excellent software.</p></div><footer class="content__footer"><div class="entry-wrapper"><p class="content__updated">This article was updated on March 23, 2025</p><div class="content__actions"><ul class="content__tag"><li><a href="./tags/blog/">Blog</a></li></ul></div><div class="content__bio bio"><div><h3 class="h4 bio__name"><a href="./authors/erik-hermansen/" rel="author">Erik Hermansen</a></h3></div></div></div><nav class="content__nav"><div class="wrapper"><div class="content__nav-inner"><div class="content__nav-next"><a href="./open-source-joy-for-publii.html" class="content__nav-link" rel="next"><div><span>Next</span> Open Source Joy for Publii</div><figure class="content__nav-image"><img src="./media/posts/12/responsive/openSourceJoy-xs.jpg" class="lazyload" loading="lazy" alt="people building a wall together" height="640" width="640"></figure></a></div></div></div></nav></footer></article></main><footer class="footer footer--glued"><div class="wrapper"><nav class="footer__nav"><ul><li><a href="./privacy-policy.html" class="al" target="_self">privacy policy</a></li><li><a href="./contact-us.html" class="al" target="_self">contact us</a></li></ul></nav><div class="footer__copyright"><p>Copyright ©2023-2025 by the respective authors. All rights reserved.</p></div><button id="backToTop" class="footer__bttop" aria-label="Back to top" title="Back to top"><svg width="20" height="20"><use xlink:href="./assets/svg/svg-map.svg#toparrow"/></svg></button></div></footer><script defer="defer" src="./assets/js/scripts.min.js?v=ffcbea6c02c8178d10092962b235a5b0"></script><script>window.publiiThemeMenuConfig={mobileMenuMode:'sidebar',animationSpeed:300,submenuWidth: 'auto',doubleClickTime:500,mobileMenuExpandableSubmenus:true,relatedContainerForOverlayMenuSelector:'.top'};</script><script>var images = document.querySelectorAll('img[loading]');
        for (var i = 0; i < images.length; i++) {
            if (images[i].complete) {
                images[i].classList.add('is-loaded');
            } else {
                images[i].addEventListener('load', function () {
                    this.classList.add('is-loaded');
                }, false);
            }
        }</script></body></html>